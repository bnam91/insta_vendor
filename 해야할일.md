<이모지 리스트>
---
<ctrl+shift+v>  
💡🏆🎯📢🚀📌🚩📍🏁⚡💬⚠️ ✅☑️🔲 ❌   
⏳⏲️⏱️  
⬅️➡️ ⬆️⬇️  ◀️▶️ 🔼🔽 
(⏲️ : 0분)
---
✅ 1순위 업무
📌 2순위 업무
☑️ 3순위 업무

몽고db :
https://cloud.mongodb.com/v2/67b53ffbba89e066f00516de#/metrics/replicaSet/67b540e4a89b973ad2a8cd52/explorer/insta09_database

cd C:\Users\USER\Desktop\github\insta_vendor && python 1-1_newfeed_crawl.py

<details>
  <summary>시나리오</summary>
    - 
    
    1. 뉴피드 크롤을 한다. ⚠️테스트중⚠️1-1 프로그램 메모리 에러 ood ➡️ 터미널킬버전 & 일반 세션횟수 조정버전 *조정버전 테스트 중
    2. 클로드 분석을 한다.
    3. 아이템 투데이를 한다. ⚠️ 스트림릿_ 아이템투데이 보여지는 데이터프레임 칼럼수정 
    4. 아이템 투데이를 보고 연락할 브랜드를 정한다. ⚠️ 데이터 필터 및, ☑️ 전화번호 몽고db에 입력기능 필요
    5. 브랜드 연락처를 찾는다. ⚠️ 협업가능하게 스프레드시트로 공유해야할지?
    6. 브랜드에 전화를 돌린다. ⚠️ 전화번호
    7. 제안서 메일을 보낸다. ☑️ 자동 메일 기능 필요
    8. 어떤 인플들에게 보낼지 분석한다.
    9. DM을 보낸다. ☑️자동 DM기능 필요
    10. 정산해준다. 끝
  </details>
<br>
<br>
🚀 20250223
<BR>
💡 수요일까지 필요할 것.

- ⚠️크롤링 몽고db에 오늘의 피드 쌓기 > 마이그레이션
- ⚠️⚠️⚠️크롤링 시 메모리 에러 문제 해결 (리오픈 케이스 oom 코드 수정해뒀는데 대기중)⚠️⚠️⚠️

- ⚠️전화를 해야됨 (연락처) >> 시트를 연다 연락처 입력할 수 있도록 한다? > 따로 몽고db가지고 파일을 만들지?
- ⚠️인플루언서 등급 분류 되어야함
- ⚠️dm 보내야함 (한시간에 10개, 스레드 나눠서 처리?)  


<BR>


---
### 처리할일
- ✅마이그레이션 완료하기 (⏲️ : 30분)
    - 컬렉션 이름들 변경하기 [안한게 있나?] > 최종테스트하기
    - 마이그레이션 할때 각 몽고크롤파일들과 제이슨 파일간의 인덱스들 잘 확인하기 (⏲️ : 30분)
- ✅ 전화를 해야됨 (연락처) >> 시트를 연다 연락처 입력할 수 있도록 한다?
- ✅ 아이템 유사도 체크 기능 넣기 1-2에 추가하기?(⏲️ : 20분)


<BR>

---
<BR>
<BR>
<BR>

# 🏆스트림릿.py
- ### 📍(검색패널)검색기능 구현 및 버튼에 기능 추가
    - ☑️브랜드 검색시 키워드 추가 검색기능 넣기


- ### 📍데이터프레임 부분
    - 📌 메모 및 전화번호 입력 기능 > 몽고에 업뎃되게?
    - ☑️ 행 삭제 기능 추가
    - ☑️ 컨트롤 z 기능 추가 (복구기능)


- ### 📍(사이드바) 버튼 별 기능 추가
    - 📌아이템 투데이 카테고리 필터 만들기(다중선택 가능하게)  
    - 📌상단에 로그인 할 ID 셀렉박스 추가  
    (텍스트파일에 디폴트 아이디 입력되어있음_  
     스트림릿실행할때마다.
     근데 셀렉박스가 바뀌면,   텍스트파일에도 아이디를 변경할 것)
    - 📌맥에서도 실행 될지 확인할 것

        ### 데이터 관리
        - ✅시트 열기 
            - 전화를 해야됨 (연락처) >> 시트를 연다 연락처 입력할 수 있도록 한다?
        - ✅시트 동기화

        ### 데이터 분석
        - ~~📌브랜드 중복체크 버튼 삭제
        - ~~📌아이템 중복체크 버튼 삭제

        ### SNS 분석
        - 📌등급 분류 [로직생각 필요] (⏲️ : 30분)

        

        ### DM 관리
       
        - 📌DM보내기 [로직생각 필요] (⏲️ : 30분)
        - 📌모달창을 통해 아이디 및 갯수, 횟수 입력? (2-1 같이)
        - 📌팔로우하기 [로직생각 필요]





### - 📍NEW 버튼, 설명서 버튼 추가
    - 📌설명서는 마크다운으로 할까? -> good idea 음 편집기가 열려야 보여지지 않나? 미리보기로 열까?
### - 📍엑셀저장 버튼
    - ✅ 보여지는 데이터프레임 그대로 시트 저장 (몽고db는 그대로 유지)

### - ✅마이그레이션 완료하기 (⏲️ : 30분)
<BR>
<BR>

# 🏆1-2 클로드 크롤 파이썬 파일 수정
- 📌아이템 유사도의 코드도 합치는거 구현

<BR>
<BR>

# 🏆3-2 아이템유사도 파일 수정
- ✅아이템유사도 어떻게 처리할지.


# 🏆추가 작업사항
### 📍샘플 데이터 관리



### 📍예상되는 에러 대응
- ☑️ 1-1파일 동시 입력상황에서 총 갯수 출력 정확성 확인
- ⚠️ 크롤링 시 메모리 에러 문제 해결 (리오픈 케이스 oom 코드 수정해뒀는데 대기중)

### 📍공통 사항
- ☑️ 백업파일 관리 방안 수립
- ☑️ 몽고db 컬렉션 이름 하드코딩 변경하기

<details>
  <summary>이전 내역 클릭해서 펼치기</summary>
  //////
  이 에러는 Selenium WebDriver와의 연결이 끊어졌거나 제대로 초기화되지 않았을 때 발생합니다. 드라이버를 재초기화하는 로직을 추가해야 할 것 같습니다.
다음과 같이 코드를 수정해보세요:
//
def initialize_driver():
    options = Options()
    options.add_argument("--start-maximized")
    options.add_experimental_option("detach", True)
    options.add_argument("disable-blink-features=AutomationControlled")
    options.add_experimental_option("excludeSwitches", ["enable-logging"])
    
    # 절대경로에서 상대경로로 변경
    user_data_dir = os.path.join(os.path.dirname(__file__), "user_data", "office_goyamedia_feed") 
    options.add_argument(f"user-data-dir={user_data_dir}")
    
    # 캐시와 임시 파일 정리 (로그인 정보 유지)
    clear_chrome_data(user_data_dir)
    
    # 추가 옵션 설정
    options.add_argument("--disable-application-cache")
    options.add_argument("--disable-cache")
    
    return webdriver.Chrome(options=options)

def run_crawler():
    while True:
        try:
            # 세션 시작 로그
            log_session_status("새로운 크롤링 세션 시작")
            
            initial_post_count = 0
            json_filepath = os.path.join(os.path.dirname(__file__), "1-1_newfeed_crawl_data.json")
            if os.path.exists(json_filepath):
                with open(json_filepath, 'r', encoding='utf-8') as f:
                    try:
                        initial_post_count = len(json.load(f))
                    except:
                        initial_post_count = 0
            
            global driver
            driver = initialize_driver()  # 드라이버 초기화
            
            for attempt in range(10):
                try:
                    print(f"\n=== 크롤링 {attempt + 1}차 시도 시작 ===")
                    result = main_crawling()
                    print(f"\n=== 크롤링 {attempt + 1}차 시도 완료 ===")
                    print(f"결과: {result}")
                    
                    if attempt < 9:
                        print("\n30초 후 다음 크롤링을 시작합니다...")
                        time.sleep(30)
                        try:
                            driver.quit()  # 기존 드라이버 종료
                            driver = initialize_driver()  # 새로운 드라이버 초기화
                        except Exception as e:
                            print(f"드라이버 재초기화 중 오류: {str(e)}")
                            driver = initialize_driver()
                except Exception as e:
                    print(f"크롤링 시도 중 오류 발생: {str(e)}")
                    try:
                        driver.quit()
                    except:
                        pass
                    driver = initialize_driver()
                    continue
            
            # 세션 종료 시 최종 게시물 수 확인
            final_post_count = 0
            if os.path.exists(json_filepath):
                with open(json_filepath, 'r', encoding='utf-8') as f:
                    try:
                        final_post_count = len(json.load(f))
                    except:
                        final_post_count = 0
            
            posts_added = final_post_count - initial_post_count
            log_session_status("크롤링 세션 종료", posts_added)
            
            try:
                driver.quit()
            except:
                pass
            
            print("\n10회의 크롤링이 완료되었습니다.")
            
            rest_time = random.uniform(300, 600)
            print(f"\n다음 크롤링 세션까지 {rest_time/60:.1f}분 대기합니다...")
            time.sleep(rest_time)
            
        except KeyboardInterrupt:
            try:
                driver.quit()
            except:
                pass
            log_session_status("사용자에 의한 크롤링 중단")
            print("\n사용자가 크롤링을 중단했습니다.")
            break
        except Exception as e:
            try:
                driver.quit()
            except:
                pass
            log_session_status(f"오류 발생: {str(e)}")
            print(f"\n크롤링 중 오류 발생: {str(e)}")
            print("5분 후 다시 시도합니다...")
            time.sleep(300)

            //
            주요 변경사항:
드라이버 초기화 함수 분리
각 크롤링 시도마다 드라이버 재초기화
예외 처리 강화 및 드라이버 종료 로직 추가
전역 변수로 driver 선언
이렇게 수정하면 연결 오류가 발생했을 때 드라이버를 자동으로 재초기화하여 크롤링을 계속할 수 있습니다.

/////
기존 JSON 파일에서 12695개의 게시물 URL을 로드했습니다.
...
현재까지 총 14314개의 게시물이 저장되었습니다.
이렇게 나오는데?  1개가 추가됐는데 왜 갯수가 달라?

//
def load_processed_posts():
    """기존 JSON 파일에서 게시물 URL들을 로드"""
    processed_posts = set()
    json_filepath = os.path.join(os.path.dirname(__file__), "1-1_newfeed_crawl_data.json")
    
    if os.path.exists(json_filepath):
        try:
            with open(json_filepath, 'r', encoding='utf-8') as f:
                existing_data = json.load(f)
                # 중복 URL 제거를 위해 set 사용
                processed_posts = {item["게시물링크"] for item in existing_data}
                print(f"\n실제 고유 게시물 수: {len(processed_posts)}")
                print(f"JSON 파일 내 총 게시물 수: {len(existing_data)}")
                if len(existing_data) != len(processed_posts):
                    print(f"중복된 게시물 수: {len(existing_data) - len(processed_posts)}")
        except Exception as e:
            print(f"기존 데이터 로드 중 오류 발생: {str(e)}")
    
    return processed_posts
///
이 수정된 코드는:
실제 고유한 게시물 수
JSON 파일에 저장된 총 게시물 수
중복된 게시물 수
를 보여줄 것입니다. 이를 통해 중복 게시물이 얼마나 있는지 확인할 수 있습니다.
문제를 해결하기 위해서는 update_json_file() 함수도 중복 체크를 강화하도록 수정해야 할 수 있습니다. 필요하시다면 해당 함수의 수정된 버전도 제공해드리겠습니다.
existing_data = []

///
현재 중복 처리 로직은 다음과 같이 작동합니다:
load_processed_posts()에서 기존 JSON 파일의 모든 게시물 URL을 set()으로 로드합니다. set을 사용하므로 여기서는 중복이 제거됩니다.
크롤링 중에는 processed_posts set을 사용해 새로운 게시물의 URL이 이미 처리되었는지 확인합니다.
update_json_file()에서는 새로운 데이터를 JSON 파일의 맨 앞에 insert(0)로 추가하지만, 여기서는 중복 체크를 하지 않고 있습니다.
문제는 update_json_file()에서 중복 체크를 하지 않고 있다는 점입니다. 즉, 메모리상의 set으로는 중복을 체크하지만, 실제 JSON 파일에 저장할 때는 중복 체크를 하지 않아서 같은 게시물이 여러 번 저장될 수 있습니다.
이 문제를 해결하려면 update_json_file() 함수에서도 게시물 URL 기준으로 중복 체크를 추가해야 합니다.

/////

5일 이상 된 게시물 발견! (현재까지 4개 발견) 5일이상된 똑같은 게시글을 보고 카운팅하는 경우는 없을까?

//

네, 현재 코드에서는 같은 오래된 게시물을 반복해서 카운팅할 가능성이 있습니다. old_post_count가 게시물 URL과 연동되어 있지 않기 때문입니다.
이를 수정하기 위해 다음과 같이 코드를 변경해보겠습니다:

def main_crawling():
    try:
        # ... existing code ...
        
        processed_posts = set()  # 이미 처리한 게시물 추적
        old_posts = set()  # 5일 이상 된 게시물의 URL을 저장하는 set
        
        while True:
            try:
                # ... existing code ...
                
                for post in posts:
                    try:
                        # ... existing code ...
                        
                        # 게시물 링크 찾기
                        post_link = post.find_element(By.CSS_SELECTOR, "a[href^='/p/']").get_attribute("href")
                        
                        # 이미 처리한 게시물이면 건너뛰기
                        if post_link in processed_posts:
                            continue
                            
                        # 게시물 작성 시간 체크
                        post_datetime = datetime.fromisoformat(post_time.replace('Z', '+00:00'))
                        current_time = datetime.now(timezone.utc)
                        time_difference = current_time - post_datetime
                        
                        days_threshold = 5
                        if time_difference.days >= days_threshold:
                            if post_link not in old_posts:  # 새로운 오래된 게시물인 경우에만
                                old_posts.add(post_link)
                                print(f"\n{days_threshold}일 이상 된 게시물 발견! (현재까지 {len(old_posts)}개 발견)")
                            
                            if len(old_posts) >= 10:
                                print(f"\n{days_threshold}일 이상 된 게시물이 10개 이상 발견되어 크롤링을 종료합니다.")
                                raise StopIteration
                        
                        # 새로운 게시물 처리
                        processed_posts.add(post_link)
                        
                        # ... rest of the code ...
                        
    주요 변경사항:
old_posts set을 추가하여 오래된 게시물의 URL을 추적
게시물이 5일 이상 되었을 때 URL이 old_posts에 없는 경우에만 카운트
len(old_posts)로 실제 고유한 오래된 게시물의 수를 체크
이렇게 하면 같은 오래된 게시물을 중복해서 카운팅하는 문제가 해결됩니다.


</details>
