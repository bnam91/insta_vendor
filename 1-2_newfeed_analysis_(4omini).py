"""
ì¸ìŠ¤íƒ€ê·¸ë¨ í”¼ë“œ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸

ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¡°:
1. 01_test_newfeed_crawl_data (ì»¬ë ‰ì…˜)
   - í¬ë¡¤ë§ëœ ì¸ìŠ¤íƒ€ê·¸ë¨ í”¼ë“œ ì›ë³¸ ë°ì´í„°
   - í¬í•¨ ì •ë³´: ê²Œì‹œë¬¼ ID, ë³¸ë¬¸ ë‚´ìš©, ì‘ì„±ì¼, ì‘ì„±ì ì •ë³´ ë“±
   - ë¶„ì„ ê²°ê³¼ í•„ë“œ: ê³µêµ¬ ì—¬ë¶€, ìƒí’ˆëª…, ë¸Œëœë“œëª…, ê³µêµ¬ ì‹œì‘ì¼/ì¢…ë£Œì¼, ìƒí’ˆ ì¹´í…Œê³ ë¦¬

2. 02_test_influencer_data (ì»¬ë ‰ì…˜)
   - ì¸í”Œë£¨ì–¸ì„œ ì •ë³´ ë°ì´í„°
   - í¬í•¨ ì •ë³´: ì¸í”Œë£¨ì–¸ì„œ ID, ê³µêµ¬ìœ ë¬´(09_is), ë¸Œëœë“œ/ìƒí’ˆ ì •ë³´
   - ë¸Œëœë“œë³„ ìƒí’ˆ ì´ë ¥ ê´€ë¦¬ (20ì¼ ì´ë‚´ ì¤‘ë³µ ì²´í¬)
   - ìƒí’ˆ ì¹´í…Œê³ ë¦¬ ì •ë³´ í¬í•¨

3. 08_test_brand_category_data (ì»¬ë ‰ì…˜)
   - ë¸Œëœë“œ ì¹´í…Œê³ ë¦¬ ê´€ë¦¬ ë°ì´í„°
   - í¬í•¨ ì •ë³´: ë¸Œëœë“œëª…, ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ì •ë³´, ë³„ì¹­(aliases), ë ˆë²¨, ìƒíƒœ

ë¡œê·¸ íŒŒì¼:
1. unregistered_influencers.log
   - ë¯¸ë“±ë¡ ì¸í”Œë£¨ì–¸ì„œ ì •ë³´ ê¸°ë¡
   - í¬í•¨ ì •ë³´: ë°œê²¬ ì‹œê°„, ì¸í”Œë£¨ì–¸ì„œëª…, ê²Œì‹œë¬¼ ë§í¬

2. influencer_update_errors.log
   - ë°ì´í„° ì²˜ë¦¬ ì¤‘ ë°œìƒí•˜ëŠ” ì˜¤ë¥˜ ë¡œê·¸
   - í¬í•¨ ì •ë³´: ì‹œê°„, ì¸í”Œë£¨ì–¸ì„œëª…, ì˜¤ë¥˜ ë‚´ìš©

3. product_similarity.log
   - ìƒí’ˆ ìœ ì‚¬ë„ ì²˜ë¦¬ ë¡œê·¸
   - í¬í•¨ ì •ë³´: ìœ ì‚¬ë„ ë¹„êµ ê³¼ì •, ê²°ê³¼, API ì‘ë‹µ ì •ë³´

4. brand_normalization.log
   - ë¸Œëœë“œ ì •ê·œí™” ì²˜ë¦¬ ë¡œê·¸
   - í¬í•¨ ì •ë³´: ë¸Œëœë“œ ìœ ì‚¬ë„ ê²€ì‚¬ ê²°ê³¼, ë³‘í•© ê³¼ì •, ìƒˆ ë¸Œëœë“œ ë“±ë¡ ì •ë³´

ì‹¤í–‰ ê²°ê³¼:
1. ì½˜ì†” ì¶œë ¥
   - ì´ ë¶„ì„í•  ê²Œì‹œê¸€ ìˆ˜ í‘œì‹œ
   - ê° ê²Œì‹œë¬¼ ë¶„ì„ ì§„í–‰ ìƒí™© (ìˆœì°¨ì ìœ¼ë¡œ í‘œì‹œ)
   - ì¤‘ë³µ ìƒí’ˆ ë°œê²¬ ì‹œ ì•Œë¦¼
   - ì¹´í…Œê³ ë¦¬ ë¶„ì„ ê²°ê³¼ í‘œì‹œ
   - ì—ëŸ¬ ë°œìƒ ì‹œ í•´ë‹¹ ê²Œì‹œë¬¼ IDì™€ ì—ëŸ¬ ë‚´ìš©
   - ì „ì²´ ë¶„ì„ ì™„ë£Œ ë©”ì‹œì§€

2. MongoDB ë°ì´í„° ì—…ë°ì´íŠ¸
   ì˜ˆì‹œ) 01_test_newfeed_crawl_data:
   {
     "author": "ì¸í”Œë£¨ì–¸ì„œëª…",
     "content": "ê²Œì‹œë¬¼ ë‚´ìš©",
     "cr_at": "2024-03-15T14:30:00",
     "09_feed": "ê³µêµ¬ì˜¤í”ˆ",
     "09_item": "ìƒí’ˆëª…",
     "09_brand": "ë¸Œëœë“œëª…",
     "open_date": "2024-03-15",
     "end_date": "2024-03-20",
     "09_item_category": "ğŸ½ì£¼ë°©ìš©í’ˆ&ì‹ê¸°",
     "09_item_category_2": "ë„ë§ˆ,ì¡°ë¦¬ë„êµ¬,ì‹ê¸°ì„¸íŠ¸",
     "processed": true
   }

3. ì¸í”Œë£¨ì–¸ì„œ ë°ì´í„° êµ¬ì¡°
   ì˜ˆì‹œ) 02_test_influencer_data:
   {
     "username": "ì¸í”Œë£¨ì–¸ì„œëª…",
     "09_is": "Y",
     "brand": [
       {
         "name": "ë¸Œëœë“œëª…",
         "category": "ì¹´í…Œê³ ë¦¬",
         "products": [
           {
             "item": "ìƒí’ˆëª…",
             "type": "ê³µêµ¬ì˜¤í”ˆ",
             "category": "ğŸ½ì£¼ë°©ìš©í’ˆ&ì‹ê¸°",
             "category2": "ë„ë§ˆ,ì¡°ë¦¬ë„êµ¬,ì‹ê¸°ì„¸íŠ¸",
             "mentioned_date": "2024-03-15T14:30:00",
             "expected_date": "2024-03-15",
             "end_date": "2024-03-20",
             "item_feed_link": "ê²Œì‹œë¬¼ë§í¬",
             "preserve": ""
           }
         ]
       }
     ]
   }

ê¸°ëŠ¥:
1. MongoDBì—ì„œ ì¸ìŠ¤íƒ€ê·¸ë¨ í”¼ë“œ ë°ì´í„°ë¥¼ ì½ì–´ì˜´
2. Claude AIë¥¼ ì‚¬ìš©í•˜ì—¬ ê° ê²Œì‹œë¬¼ ë¶„ì„:
   - ê³µêµ¬ ê²Œì‹œë¬¼ ì—¬ë¶€ íŒë‹¨ (ê³µêµ¬ì˜ˆê³ /ê³µêµ¬ì˜¤í”ˆ/ê³µêµ¬ë¦¬ë§ˆì¸ë“œ/í™•ì¸í•„ìš”/N)
   - ìƒí’ˆëª…, ë¸Œëœë“œëª… ì¶”ì¶œ
   - ê³µêµ¬ ì‹œì‘ì¼/ì¢…ë£Œì¼ ì¶”ì¶œ
3. Gemini AIë¥¼ ì‚¬ìš©í•˜ì—¬:
   - ê° ê²Œì‹œë¬¼ 1íšŒ ë¶„ì„ í›„ ê²°ê³¼ ì„ íƒ
   - ìƒí’ˆ ì¹´í…Œê³ ë¦¬ ë° ì„œë¸Œ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
4. OpenAI APIë¥¼ ì‚¬ìš©í•œ ìƒí’ˆ ìœ ì‚¬ë„ ì¸¡ì •:
   - 20ì¼ ì´ë‚´ ìœ ì‚¬ ìƒí’ˆ ì¤‘ë³µ ì²´í¬
   - 70% ì´ìƒ ìœ ì‚¬ë„ ì‹œ ì¤‘ë³µìœ¼ë¡œ íŒë‹¨
5. ë¸Œëœë“œ ì •ê·œí™” ì²˜ë¦¬:
   - Jaro-Winkler ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•œ ë¸Œëœë“œëª… ìœ ì‚¬ë„ ì¸¡ì •
   - ìœ ì‚¬ ë¸Œëœë“œ ìë™ ë³‘í•© ë° ë³„ì¹­ ê´€ë¦¬
   - ìƒˆë¡œìš´ ë¸Œëœë“œ ìë™ ë“±ë¡
6. ë¶„ì„ ê²°ê³¼ë¥¼ MongoDBì— ìë™ ì—…ë°ì´íŠ¸
7. ì¸í”Œë£¨ì–¸ì„œ ë°ì´í„° ìë™ ì—…ë°ì´íŠ¸

ë°ì´í„° ì²˜ë¦¬ ê·œì¹™:
1. ê³µêµ¬ ë¶„ë¥˜:
   - ê³µêµ¬ì˜ˆê³ : í–¥í›„ ê³µêµ¬ ì˜ˆì • ê²Œì‹œë¬¼
   - ê³µêµ¬ì˜¤í”ˆ: í˜„ì¬ ì§„í–‰ ì¤‘ì¸ ê³µêµ¬
   - ê³µêµ¬ë¦¬ë§ˆì¸ë“œ: ë§ˆê° ì„ë°• ê³µêµ¬
   - í™•ì¸í•„ìš”: ê³µêµ¬ ì—¬ë¶€ ë¶ˆëª…í™•
   - N: ê³µêµ¬ ì•„ë‹˜

2. ë‚ ì§œ ì²˜ë¦¬:
   - YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ì €ì¥
   - 'ë‹¹ì¼' í‘œì‹œëŠ” ê²Œì‹œë¬¼ ì‘ì„±ì¼ë¡œ ë³€í™˜
   - ì—°ë„ ë¯¸ì§€ì •ì‹œ í˜„ì¬ ì—°ë„ ì‚¬ìš©

3. ë¸Œëœë“œ ì²˜ë¦¬:
   - ë¸Œëœë“œëª… ìœ ì‚¬ë„ 0.85 ì´ìƒ ì‹œ ìœ ì‚¬ ë¸Œëœë“œë¡œ íŒë‹¨
   - ìœ ì‚¬ ë¸Œëœë“œ ë°œê²¬ ì‹œ ìë™ ë³‘í•© ë° ë³„ì¹­ í†µí•©
   - ëŒ€í‘œ ë¸Œëœë“œ ì„ ì • ê¸°ì¤€: ë³„ì¹­ ìˆ˜, í•œê¸€ í¬í•¨, ê³µë°± ì—†ìŒ, ì´ë¦„ ê¸¸ì´
   - ë¯¸ë“±ë¡ ë¸Œëœë“œ ìë™ ë“±ë¡ (status: 'ready')
   - ë³µìˆ˜ ë¸Œëœë“œì¸ ê²½ìš° 'ë³µí•©ìƒí’ˆ'ìœ¼ë¡œ ì²˜ë¦¬

4. ìƒí’ˆ ì¤‘ë³µ ì²´í¬:
   - ë™ì¼ ìƒí’ˆ 20ì¼ ì´ë‚´ ì¤‘ë³µ ë“±ë¡ ë°©ì§€
   - OpenAI APIë¥¼ í†µí•œ ìƒí’ˆëª… ìœ ì‚¬ë„ ì¸¡ì •
   - 70% ì´ìƒ ìœ ì‚¬ë„ ì‹œ ì¤‘ë³µìœ¼ë¡œ íŒë‹¨

5. ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜:
   - OpenAIë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ ë¶„ì„
   - ì£¼ ì¹´í…Œê³ ë¦¬(ì´ëª¨ì§€ í¬í•¨)ì™€ ì„œë¸Œ ì¹´í…Œê³ ë¦¬ êµ¬ë¶„
   - ì£¼ë°©ìš©í’ˆ, ìƒí™œìš©í’ˆ, ì‹í’ˆ, ë·°í‹°, ìœ ì•„, ì˜ë¥˜, ì „ìì œí’ˆ ë“± ë¶„ë¥˜

ì˜¤ë¥˜ ì²˜ë¦¬:
- ê°œë³„ ê²Œì‹œë¬¼ ì²˜ë¦¬ ì‹¤íŒ¨ì‹œ ë‹¤ìŒ ê²Œì‹œë¬¼ë¡œ ì§„í–‰
- ì˜¤ë¥˜ ìƒì„¸ ë‚´ìš© ë¡œê·¸ íŒŒì¼ì— ê¸°ë¡
- ë¯¸ë“±ë¡ ì¸í”Œë£¨ì–¸ì„œ ë³„ë„ ë¡œê·¸ ê´€ë¦¬
- API í˜¸ì¶œ ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜

ì£¼ì˜ì‚¬í•­:
- API í˜¸ì¶œ ì œí•œ ë°©ì§€ë¥¼ ìœ„í•œ ë”œë ˆì´ í¬í•¨
- ì´ë¯¸ ì²˜ë¦¬ëœ ë°ì´í„°ëŠ” ê±´ë„ˆëœ€
- ë¸Œëœë“œëª… ëˆ„ë½ì‹œ 'í™•ì¸í•„ìš”'ë¡œ ì„¤ì •
- ìœ ì‚¬ë„ ì¸¡ì • API íƒ€ì„ì•„ì›ƒ 30ì´ˆ ì„¤ì •

ë°ì´í„°ë² ì´ìŠ¤:
- MongoDB Atlas ì‚¬ìš©
- ë°ì´í„°ë² ì´ìŠ¤ëª…: insta09_database
- ì»¬ë ‰ì…˜:
  * 01_test_newfeed_crawl_data: í¬ë¡¤ë§ëœ í”¼ë“œ ë°ì´í„°
  * 02_test_influencer_data: ì¸í”Œë£¨ì–¸ì„œ ì •ë³´
  * 08_test_brand_category_data: ë¸Œëœë“œ ì¹´í…Œê³ ë¦¬ ê´€ë¦¬

ì‚¬ìš© API:
- Claude API: ê²Œì‹œë¬¼ ë‚´ìš© ë¶„ì„ (ê³µêµ¬ ì—¬ë¶€, ìƒí’ˆëª…, ë¸Œëœë“œëª…, ë‚ ì§œ ì¶”ì¶œ)
- Gemini API: ìƒí’ˆ ì¹´í…Œê³ ë¦¬ ë¶„ì„ (ì£¼ ì¹´í…Œê³ ë¦¬, ì„œë¸Œ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜)
- OpenAI API: ìƒí’ˆëª… ìœ ì‚¬ë„ ì¸¡ì • (ì¤‘ë³µ ì²´í¬)
"""


import anthropic
import time
import json
from datetime import datetime
import os
from dotenv import load_dotenv
from pymongo import MongoClient
from pymongo.server_api import ServerApi
import re
import openai
from jellyfish import jaro_winkler_similarity
import logging

def get_mongodb_connection():
    uri = "mongodb+srv://coq3820:JmbIOcaEOrvkpQo1@cluster0.qj1ty.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0"
    client = MongoClient(uri, server_api=ServerApi('1'))
    
    try:
        # ì—°ê²° í…ŒìŠ¤íŠ¸
        client.admin.command('ping')
        print("MongoDB ì—°ê²° ì„±ê³µ!")
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì„ íƒ
        db = client['insta09_database']
        
        # ì»¬ë ‰ì…˜ ë§¤í•‘
        collections = {
            'feeds': db['01_main_newfeed_crawl_data'],
            'influencers': db['02_main_influencer_data'],
            'brands': db['08_main_brand_category_data']
        }
        
        return client, collections
    except Exception as e:
        print(f"MongoDB ì—°ê²° ì‹¤íŒ¨: {str(e)}")
        raise

def update_influencer_data(item, collections):
    try:
        # ë¸Œëœë“œ ì¹´í…Œê³ ë¦¬ ë°ì´í„° ë¡œë“œ
        brands_collection = collections['brands']
        
        # ì¸í”Œë£¨ì–¸ì„œ ë°ì´í„° ì°¾ê¸°
        influencers_collection = collections['influencers']
        influencer = influencers_collection.find_one({'username': item['author']})
        
        # ë¡œê·¸ ë””ë ‰í† ë¦¬ ì„¤ì •
        log_dir = setup_log_directory()
        
        # ë¸Œëœë“œëª… ì •ê·œí™” ë° ì¹´í…Œê³ ë¦¬ ì°¾ê¸° í•¨ìˆ˜
        def normalize_brand(brand_name):
            brand_info = brands_collection.find_one({
                '$or': [
                    {'name': brand_name},
                    {'aliases': brand_name}
                ]
            })
            
            if brand_info:
                return {
                    'name': brand_info['name'],
                    'category': brand_info.get('category', '')  # get ë©”ì„œë“œë¡œ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°
                }
            
            # ë¸Œëœë“œê°€ ì—†ëŠ” ê²½ìš° ìƒˆë¡œ ì¶”ê°€
            if brand_name and brand_name != 'í™•ì¸í•„ìš”':
                new_brand = {
                    'name': brand_name,
                    'category': '',
                    'aliases': [],
                    'level': '',
                    'status': 'ready'
                }
                brands_collection.insert_one(new_brand)
                return {
                    'name': brand_name,
                    'category': ''
                }
            return {
                'name': brand_name,
                'category': ''
            }

        if not influencer:
            # ë¯¸ë“±ë¡ ì¸í”Œë£¨ì–¸ì„œ ë¡œê·¸ ê¸°ë¡
            log_message = f"[{item['cr_at']}] ë¯¸ë“±ë¡ ì¸í”Œë£¨ì–¸ì„œ ë°œê²¬: {item['author']} (ê²Œì‹œë¬¼ë§í¬: {item['post_url']})\n"
            with open(os.path.join(log_dir, 'unregistered_influencers.log'), 'a', encoding='utf-8') as log_file:
                log_file.write(log_message)
            print(f"ë¯¸ë“±ë¡ ì¸í”Œë£¨ì–¸ì„œ ë¡œê·¸ ê¸°ë¡: {item['author']}")
            return

        # ê³µêµ¬ìœ ë¬´ ì—…ë°ì´íŠ¸
        if influencer.get('09_is') != 'Y':
            influencers_collection.update_one(
                {'username': item['author']},
                {'$set': {'09_is': 'Y'}}
            )

        brands = item['09_brand'].split(', ')
        for brand in brands:
            # ì™¸ë¶€ normalize_brand í•¨ìˆ˜ ì‚¬ìš©, ì‘ì„±ì ì •ë³´ ì „ë‹¬
            brand_info = normalize_brand(brand)
            normalized_brand_name = brand_info['name']
            
            # ë¸Œëœë“œ ì •ë³´ ì—…ë°ì´íŠ¸
            brand_data = {
                'name': normalized_brand_name,
                'category': brand_info['category'],
                'products': [{
                    'item': item['09_item'],
                    'type': item['09_feed'],
                    'category': item.get('09_item_category', ''),
                    'category2': item.get('09_item_category_2', ''),  # ì„œë¸Œ ì¹´í…Œê³ ë¦¬ ì •ë³´ ì¶”ê°€
                    'mentioned_date': item['cr_at'],
                    'expected_date': item['open_date'],
                    'end_date': item['end_date'],
                    'item_feed_link': item['post_url'],
                    'preserve': ''
                }]
            }

            # ì¤‘ë³µ ì²´í¬ ë° ì—…ë°ì´íŠ¸
            existing_brand = influencers_collection.find_one({
                'username': item['author'],
                'brand': {
                    '$elemMatch': {
                        'products': {
                            '$elemMatch': {
                                'mentioned_date': item['cr_at'],
                                'item_feed_link': item['post_url']
                            }
                        }
                    }
                }
            })

            if existing_brand:
                print(f"ë™ì¼ ê²Œì‹œê¸€ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {item['post_url']}")
                continue

            # ê¸°ì¡´ ë¸Œëœë“œ ê²€ìƒ‰
            existing_brand = influencers_collection.find_one({
                'username': item['author'],
                'brand.name': normalized_brand_name
            })

            if existing_brand:
                # ì¤‘ë³µ ì²´í¬ ë¡œì§ (20ì¼ ì´ë‚´)
                products = None
                for b in existing_brand['brand']:
                    if b['name'] == normalized_brand_name:
                        products = b['products']
                        break
                
                if not products:
                    continue
                    
                product_exists = False
                
                # ìœ ì‚¬ë„ ì²˜ë¦¬ ë¡œê·¸ ì‹œì‘
                with open(os.path.join(log_dir, 'product_similarity.log'), 'a', encoding='utf-8') as log_file:
                    log_file.write(f"\n===== ìœ ì‚¬ë„ ì²˜ë¦¬ ì‹œì‘: {item['author']} - {normalized_brand_name} - {item['09_item']} =====\n")
                
                for product in products:
                    existing_date = datetime.strptime(product['mentioned_date'].split('T')[0], '%Y-%m-%d')
                    new_date = datetime.strptime(item['cr_at'].split('T')[0], '%Y-%m-%d')
                    date_diff = abs((new_date - existing_date).days)
                    
                    # ë‚ ì§œ ì°¨ì´ê°€ 20ì¼ ì´ë‚´ì¸ ê²½ìš°ì—ë§Œ ìœ ì‚¬ë„ ê²€ì‚¬
                    if date_diff <= 20:
                        # ìœ ì‚¬ë„ ì²˜ë¦¬ ë¡œê·¸
                        with open(os.path.join(log_dir, 'product_similarity.log'), 'a', encoding='utf-8') as log_file:
                            log_file.write(f"ë¹„êµ ëŒ€ìƒ: {product['item']} (ì–¸ê¸‰ì¼: {product['mentioned_date']})\n")
                            log_file.write(f"ë‚ ì§œ ì°¨ì´: {date_diff}ì¼\n")
                        
                        # ìœ ì‚¬ë„ ì¸¡ì •
                        similarity_score, input_tokens, output_tokens = calculate_similarity(product['item'], item['09_item'], log_dir)
                        
                        # ìœ ì‚¬ë„ ê²°ê³¼ ë¡œê·¸
                        with open(os.path.join(log_dir, 'product_similarity.log'), 'a', encoding='utf-8') as log_file:
                            log_file.write(f"ìœ ì‚¬ë„ ì ìˆ˜: {similarity_score}%\n")
                        
                        # ìœ ì‚¬ë„ê°€ 70% ì´ìƒì´ë©´ ì¤‘ë³µìœ¼ë¡œ íŒë‹¨
                        if similarity_score >= 70:
                            product_exists = True
                            message = f"ì¤‘ë³µ ìƒí’ˆ ë°œê²¬: {item['09_item']} - ê¸°ì¡´:{existing_date.date()} ì‹ ê·œ:{new_date.date()} (ì°¨ì´: {date_diff}ì¼, ìœ ì‚¬ë„: {similarity_score}%)"
                            print(message)
                            
                            # ì¤‘ë³µ ë°œê²¬ ë¡œê·¸
                            with open(os.path.join(log_dir, 'product_similarity.log'), 'a', encoding='utf-8') as log_file:
                                log_file.write(f"ê²°ê³¼: {message}\n")
                            
                            break
                
                # ìœ ì‚¬ë„ ì²˜ë¦¬ ë¡œê·¸ ì¢…ë£Œ
                with open(os.path.join(log_dir, 'product_similarity.log'), 'a', encoding='utf-8') as log_file:
                    if not product_exists:
                        log_file.write("ê²°ê³¼: ì¤‘ë³µ ìƒí’ˆ ì—†ìŒ\n")
                    log_file.write(f"===== ìœ ì‚¬ë„ ì²˜ë¦¬ ì¢…ë£Œ =====\n")

                if not product_exists:
                    influencers_collection.update_one(
                        {'username': item['author'], 'brand.name': normalized_brand_name},
                        {'$push': {'brand.$.products': brand_data['products'][0]}}
                    )
            else:
                # ìƒˆ ë¸Œëœë“œ ì¶”ê°€
                influencers_collection.update_one(
                    {'username': item['author']},
                    {'$push': {'brand': brand_data}}
                )

        print(f"ì¸í”Œë£¨ì–¸ì„œ ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ: {item['author']}")

    except Exception as e:
        log_dir = setup_log_directory()
        error_message = f"[{item['cr_at']}] ì˜¤ë¥˜ ë°œìƒ - ì¸í”Œë£¨ì–¸ì„œ: {item['author']}, ì˜¤ë¥˜: {str(e)}\n"
        with open(os.path.join(log_dir, 'influencer_update_errors.log'), 'a', encoding='utf-8') as error_file:
            error_file.write(error_message)
        print(f"ì¸í”Œë£¨ì–¸ì„œ ë°ì´í„° ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

def calculate_similarity(item1, item2, log_dir=None):
    """ë‘ ìƒí’ˆì˜ ìœ ì‚¬ë„ ê³„ì‚°"""
    try:
        # ë¡œê·¸ ë””ë ‰í† ë¦¬ê°€ ì „ë‹¬ë˜ì§€ ì•Šì€ ê²½ìš° ì„¤ì •
        if log_dir is None:
            log_dir = setup_log_directory()
            
        # ìœ ì‚¬ë„ ì²˜ë¦¬ ë¡œê·¸
        with open(os.path.join(log_dir, 'product_similarity.log'), 'a', encoding='utf-8') as log_file:
            log_file.write(f"API ìš”ì²­ ì‹œì‘: {item1} vs {item2}\n")
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        
        # ìœ ì‚¬ë„ ì¸¡ì • API í˜¸ì¶œ (íƒ€ì„ì•„ì›ƒ 30ì´ˆ ì„¤ì •)
        similarity_response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": """
                ë‘ ìƒí’ˆëª…ì˜ ìœ ì‚¬ë„ë¥¼ 0ì—ì„œ 100 ì‚¬ì´ì˜ ìˆ«ìë¡œ í‰ê°€í•´ì£¼ì„¸ìš”.
                í‰ê°€ ê¸°ì¤€:
                - 100: ì™„ì „íˆ ë™ì¼í•œ ìƒí’ˆ
                - 90-99: ê°™ì€ ìƒí’ˆì´ì§€ë§Œ í‘œê¸°ë²•ë§Œ ë‹¤ë¥¸ ê²½ìš°
                - 80-89: ê°™ì€ ìƒí’ˆì´ì§€ë§Œ ë¸Œëœë“œëª…ì´ë‚˜ ë¶€ê°€ ì„¤ëª…ì´ ìˆëŠ” ê²½ìš°
                - 70-79: ê°™ì€ ìƒí’ˆì´ì§€ë§Œ ì—ë””ì…˜ì´ë‚˜ ì˜µì…˜ì´ ë‹¤ë¥¸ ê²½ìš°
                - 0: ì „í˜€ ë‹¤ë¥¸ ìƒí’ˆ
                ìˆ«ìë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”.
                """},
                {"role": "user", "content": f"ì²« ë²ˆì§¸ ìƒí’ˆëª…: {item1}\në‘ ë²ˆì§¸ ìƒí’ˆëª…: {item2}"}
            ],
            timeout=30  # 30ì´ˆ íƒ€ì„ì•„ì›ƒ ì„¤ì •
        )
        
        # í† í° ì‚¬ìš©ëŸ‰ ê³„ì‚°
        input_tokens = similarity_response.usage.prompt_tokens
        output_tokens = similarity_response.usage.completion_tokens
        
        # ì‘ë‹µì—ì„œ ìœ ì‚¬ë„ ì ìˆ˜ ì¶”ì¶œ
        similarity_score = int(similarity_response.choices[0].message.content.strip())
        
        # ìœ ì‚¬ë„ ì²˜ë¦¬ ë¡œê·¸
        with open(os.path.join(log_dir, 'product_similarity.log'), 'a', encoding='utf-8') as log_file:
            log_file.write(f"API ì‘ë‹µ ìˆ˜ì‹ : ìœ ì‚¬ë„ {similarity_score}%, ì…ë ¥ í† í°: {input_tokens}, ì¶œë ¥ í† í°: {output_tokens}\n")
        
        return similarity_score, input_tokens, output_tokens
        
    except Exception as e:
        error_msg = f"ìœ ì‚¬ë„ ì¸¡ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        print(error_msg)
        
        # ë¡œê·¸ ë””ë ‰í† ë¦¬ê°€ ì „ë‹¬ë˜ì§€ ì•Šì€ ê²½ìš° ì„¤ì •
        if log_dir is None:
            log_dir = setup_log_directory()
            
        # ì˜¤ë¥˜ ë¡œê·¸
        with open(os.path.join(log_dir, 'product_similarity.log'), 'a', encoding='utf-8') as log_file:
            log_file.write(f"ì˜¤ë¥˜: {error_msg}\n")
        
        return 0, 0, 0  # ì˜¤ë¥˜ ë°œìƒ ì‹œ 0 ë°˜í™˜

def analyze_product_category(product_name):
    """
    OpenAI GPT ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìƒí’ˆëª…ì„ ë¶„ì„í•˜ê³  ì í•©í•œ ì¹´í…Œê³ ë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Args:
        product_name (str): ë¶„ì„í•  ìƒí’ˆëª…
        
    Returns:
        tuple: (ì£¼ ì¹´í…Œê³ ë¦¬, ì„œë¸Œ ì¹´í…Œê³ ë¦¬)
    """
    retry_count = 0
    retry_delay = 10  # ì´ˆê¸° ëŒ€ê¸° ì‹œê°„

    while True:  # ë¬´í•œ ë£¨í”„ ìœ ì§€
        try:
            # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
            
            # ì¹´í…Œê³ ë¦¬ ì •ì˜
            categories = """
            ğŸ½ì£¼ë°©ìš©í’ˆ&ì‹ê¸°
            ë„ë§ˆ,ì¡°ë¦¬ë„êµ¬,ì‹ê¸°ì„¸íŠ¸ - ë„ë§ˆ, ì¹¼, ì¡°ë¦¬ë„êµ¬, ê·¸ë¦‡, ì ‘ì‹œ, ì»µ, ìˆ˜ì €, ì‹ê¸°ì„¸íŠ¸
            ëƒ„ë¹„&í”„ë¼ì´íŒ¬ - ëƒ„ë¹„, í”„ë¼ì´íŒ¬, ì›, ì°œê¸°, ì••ë ¥ì†¥, ëšë°°ê¸°
            ì£¼ë°©ê°€ì „ - ì—ì–´í”„ë¼ì´ì–´, ì˜¤ë¸, í† ìŠ¤í„°, ë¸”ë Œë”, ë¯¹ì„œê¸°, ì „ê¸°í¬íŠ¸, ì»¤í”¼ë¨¸ì‹ 
            ë°€í/ë³´ê´€ ìš©ê¸° - ë°€íìš©ê¸°, ë³´ê´€ìš©ê¸°, ìœ ë¦¬ìš©ê¸°, ìŠ¤í…Œì¸ë¦¬ìŠ¤ ìš©ê¸°, ì§„ê³µìš©ê¸°
            ê¸°íƒ€ - ë¶„ë¥˜ê°€ ì•ˆëœ ê²ƒ

            ğŸ›‹ìƒí™œìš©í’ˆ&ê°€ì „
            ì²­ì†Œê¸°&ì„¸ì²™ê¸° - ì²­ì†Œê¸°, ë¡œë´‡ì²­ì†Œê¸°, í•¸ë””ì²­ì†Œê¸°, ìŠ¤íŒ€ì²­ì†Œê¸°, ì„¸ì²™ê¸°
            ì„¸íƒ/ìš•ì‹¤ìš©í’ˆ - ì„¸ì œ, ì„¬ìœ ìœ ì—°ì œ, ì„¸íƒì„¸ì œ, ì£¼ë°©ì„¸ì œ, ê³ ë¬´ì¥ê°‘, ìš•ì‹¤ìš©í’ˆ, ìˆ˜ê±´
            ì¡°ëª…&ê°€êµ¬ - ì¡°ëª…, ìŠ¤íƒ ë“œ, ì±…ìƒ, ì˜ì, ì¹¨ëŒ€, ì†ŒíŒŒ, í…Œì´ë¸”, ìˆ˜ë‚©ì¥
            ì¹¨êµ¬&ì»¤íŠ¼ - ì´ë¶ˆ, ë² ê°œ, ì¹¨ëŒ€íŒ¨ë“œ, ë§¤íŠ¸ë¦¬ìŠ¤, ì»¤íŠ¼, ë¸”ë¼ì¸ë“œ, ëŸ¬ê·¸, ì¹´í˜íŠ¸
            êµ¬ê°•ì¼€ì–´ - ì¹˜ì•½, ì¹«ì†”, êµ¬ê°•ì„¸ì •ì œ, ì¹˜ì‹¤, êµ¬ê°•ìŠ¤í”„ë ˆì´
            ê¸°íƒ€ - ë¶„ë¥˜ê°€ ì•ˆëœ ê²ƒ
            
            ğŸ¥¦ì‹í’ˆ&ê±´ê°•ì‹í’ˆ
            ê±´ê°•ìŒë£Œ&ì°¨ - ì°¨, ê±´ê°•ìŒë£Œ, ì½¤ë¶€ì°¨, ì‹í˜œ, ìˆ˜ì œì²­, ê³¼ì¼ì²­, ì‹ì´ˆ
            ê°„í¸ì‹&ì¡°ë¯¸ë£Œ - ê°„í¸ì‹, ì¦‰ì„ì‹í’ˆ, ì†ŒìŠ¤, ì¡°ë¯¸ë£Œ, ì–‘ë…, ê³°íƒ•, êµ­, ì°Œê°œ
            ìŠ¤ë‚µ&ê°„ì‹ - ê³¼ì, ì¿ í‚¤, ì´ˆì½œë¦¿, ì ¤ë¦¬, ê²¬ê³¼ë¥˜, ê·¸ë˜ë†€ë¼, ì‹œë¦¬ì–¼
            ì¶•ì‚°&ìˆ˜ì‚°ë¬¼ - ìœ¡ë¥˜, í•´ì‚°ë¬¼, ìƒì„ , ê³„ë€, ìš°ìœ , ì¹˜ì¦ˆ, ìš”ê±°íŠ¸
            ê³¼ì¼&ì‹ ì„ ì‹í’ˆ - ê³¼ì¼, ì±„ì†Œ, ìƒëŸ¬ë“œ, ë‚˜ë¬¼, ë²„ì„¯
            ê±´ê°•ë³´ì¡°ì œ - ë¹„íƒ€ë¯¼, ì˜ì–‘ì œ, í”„ë¡œë°”ì´ì˜¤í‹±ìŠ¤, ì½œë¼ê², ë‹¨ë°±ì§ˆ ë³´ì¶©ì œ
            ê¸°íƒ€ - ë¶„ë¥˜ê°€ ì•ˆëœ ê²ƒ

            ğŸ§´ë·°í‹°&í—¬ìŠ¤
            í—¤ì–´&ë°”ë”” - ìƒ´í‘¸, íŠ¸ë¦¬íŠ¸ë¨¼íŠ¸, ë°”ë””ì›Œì‹œ, ë°”ë””ë¡œì…˜, í•¸ë“œí¬ë¦¼, ë°”ë””ìŠ¤í¬ëŸ½
            ìŠ¤í‚¨ì¼€ì–´&í™”ì¥í’ˆ - ìŠ¤í‚¨, ë¡œì…˜, ì—ì„¼ìŠ¤, í¬ë¦¼, ë§ˆìŠ¤í¬íŒ©, ì„ í¬ë¦¼, ë©”ì´í¬ì—…
            í—¬ìŠ¤&í”¼íŠ¸ë‹ˆìŠ¤ - ìš´ë™ê¸°êµ¬, ìš”ê°€ë§¤íŠ¸, ë¤ë²¨, ë³´ì¡°ì œ, í”„ë¡œí‹´, ë‹¤ì´ì–´íŠ¸ì‹í’ˆ
            ê¸°íƒ€ - ë¶„ë¥˜ê°€ ì•ˆëœ ê²ƒ

            ğŸ‘¶ìœ ì•„&êµìœ¡
            ìœ ì•„ê°€êµ¬ & ì¹¨êµ¬ - ìœ ì•„ì¹¨ëŒ€, ìœ ì•„ì±…ìƒ, ìœ ì•„ì˜ì, ìœ ì•„ì´ë¶ˆ, ìœ ì•„ë² ê°œ
            êµìœ¡&ì™„êµ¬ - ì±…, êµêµ¬, ì¥ë‚œê°, í¼ì¦, ë¸”ë¡, ë³´ë“œê²Œì„, ì¸í˜•
            ê¸°íƒ€ - ë¶„ë¥˜ê°€ ì•ˆëœ ê²ƒ

            ğŸ‘—ì˜ë¥˜&ì¡í™”
            ì˜ë¥˜&ì‹ ë°œ - ì˜·, ì˜ë¥˜, íŒ¨ë”©, ì½”íŠ¸, ìì¼“, í‹°ì…”ì¸ , ë°”ì§€, ì‹ ë°œ, ìŠ¬ë¦¬í¼
            ê°€ë°©&ì•¡ì„¸ì„œë¦¬ - ê°€ë°©, ë°±íŒ©, ì§€ê°‘, ë²¨íŠ¸, ëª¨ì, ìŠ¤ì¹´í”„, ëª©ê±¸ì´, ê·€ê±¸ì´
            ê¸°íƒ€ - ë¶„ë¥˜ê°€ ì•ˆëœ ê²ƒ

            ğŸš—ê¸°íƒ€
            ì „ìê¸°ê¸° - TV, ìŠ¤í”¼ì»¤, ì´ì–´í°, í—¤ë“œí°, ì¶©ì „ê¸°, ë…¸íŠ¸ë¶, íƒœë¸”ë¦¿
            ê¸°íƒ€ - ë¶„ë¥˜ê°€ ì•ˆëœ ê²ƒ
            """
            
            # í”„ë¡¬í”„íŠ¸ ì‘ì„±
            prompt = f"""
            ë‹¹ì‹ ì€ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì •í™•í•˜ê³  ì¼ê´€ëœ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ê°€ ë‹¹ì‹ ì˜ ì£¼ìš” ì—…ë¬´ì…ë‹ˆë‹¤.
            
            ë‹¤ìŒ ìƒí’ˆëª…ì„ ë¶„ì„í•˜ì—¬ ê°€ì¥ ì í•©í•œ ì¹´í…Œê³ ë¦¬ì™€ ì„œë¸Œì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”:
            
            ìƒí’ˆëª…: {product_name}
            
            ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ ëª©ë¡ì—ì„œë§Œ ì„ íƒí•˜ì„¸ìš”:
            {categories}
            
            ë¶„ë¥˜ ê·œì¹™:
            1. ìƒí’ˆì˜ ì£¼ìš” ê¸°ëŠ¥ê³¼ ìš©ë„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.
            2. ì—¬ëŸ¬ ì¹´í…Œê³ ë¦¬ì— ê±¸ì³ìˆëŠ” ìƒí’ˆì€ ì£¼ëœ ìš©ë„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.
            3. ë¸Œëœë“œëª…ì€ ë¬´ì‹œí•˜ê³  ìƒí’ˆ ìì²´ì˜ íŠ¹ì„±ìœ¼ë¡œë§Œ íŒë‹¨í•˜ì„¸ìš”.
            4. ìƒí’ˆëª…ë§Œìœ¼ë¡œ ë¶„ë¥˜ê°€ ì–´ë µê±°ë‚˜ ì¹´í…Œê³ ë¦¬ ëª©ë¡ì— ëª…í™•íˆ í¬í•¨ë˜ì§€ ì•ŠëŠ” ê²½ìš° 'ê¸°íƒ€' ì¹´í…Œê³ ë¦¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
            5. ì‹í’ˆ ê´€ë ¨ ìƒí’ˆì€ ìµœëŒ€í•œ êµ¬ì²´ì ì¸ ì‹í’ˆ ì„œë¸Œì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.
            6. ë¶„ë¥˜ê°€ ì•ˆëœë‹¤ë©´ ê¸°íƒ€ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.
            
            ì£¼ ì¹´í…Œê³ ë¦¬ëŠ” ì´ëª¨ì§€ê°€ í¬í•¨ëœ ëŒ€ë¶„ë¥˜(ì˜ˆ: ğŸ½ì£¼ë°©ìš©í’ˆ&ì‹ê¸°)ë¥¼ ì„ íƒí•˜ê³ ,
            ì„œë¸Œ ì¹´í…Œê³ ë¦¬ëŠ” í•´ë‹¹ ì£¼ ì¹´í…Œê³ ë¦¬ ì•„ë˜ì˜ ì†Œë¶„ë¥˜(ì˜ˆ: ë„ë§ˆ,ì¡°ë¦¬ë„êµ¬,ì‹ê¸°ì„¸íŠ¸)ë¥¼ ì„ íƒí•˜ì„¸ìš”.
            
            ì˜ˆì‹œ:
            1. "íŠ¸ë¦¬ì³ë„ë§ˆ+ì§„ê³µë°§ë“œì„¸íŠ¸" â†’ ğŸ½ì£¼ë°©ìš©í’ˆ&ì‹ê¸° / ë„ë§ˆ,ì¡°ë¦¬ë„êµ¬,ì‹ê¸°ì„¸íŠ¸
            2. "í”„ë¦¬ë¯¸ì—„ ë‹ˆíŠ¸ë¦´ ê³ ë¬´ì¥ê°‘" â†’ ğŸ›‹ìƒí™œìš©í’ˆ&ê°€ì „ / ì„¸íƒ/ìš•ì‹¤ìš©í’ˆ
            3. "í¬ë¹™ì´ë¶ˆ (ê³ ë°€ë„ ëª¨ë‹¬/ë©´ ì°¨ë µì´ë¶ˆ, ì¹¨ëŒ€íŒ¨ë“œ, ë² ê°œì»¤ë²„)" â†’ ğŸ›‹ìƒí™œìš©í’ˆ&ê°€ì „ / ì¹¨êµ¬&ì»¤íŠ¼
            4. "ì„¸íƒì„¸ì œ, ì„¬ìœ ìœ ì—°ì œ, ì£¼ë°©ì„¸ì œ" â†’ ğŸ›‹ìƒí™œìš©í’ˆ&ê°€ì „ / ì„¸íƒ/ìš•ì‹¤ìš©í’ˆ
            5. "í™”ì¥í’ˆ 3ì¢…ì„¸íŠ¸" â†’ ğŸ§´ë·°í‹°&í—¬ìŠ¤ / ìŠ¤í‚¨ì¼€ì–´&í™”ì¥í’ˆ
            6. "í´ë¦°í†¡ ê³¼ì¼ì•¼ì±„ ì£¼ìŠ¤" â†’ ğŸ¥¦ì‹í’ˆ&ê±´ê°•ì‹í’ˆ / ê±´ê°•ìŒë£Œ&ì°¨
            7. "ìœ ê¸°ë† ê²¬ê³¼ë¥˜ ì„ ë¬¼ì„¸íŠ¸" â†’ ğŸ¥¦ì‹í’ˆ&ê±´ê°•ì‹í’ˆ / ìŠ¤ë‚µ&ê°„ì‹
            8. "í•œìš° ì„ ë¬¼ì„¸íŠ¸" â†’ ğŸ¥¦ì‹í’ˆ&ê±´ê°•ì‹í’ˆ / ì¶•ì‚°&ìˆ˜ì‚°ë¬¼
            9. "ë©€í‹°ë¹„íƒ€ë¯¼" â†’ ğŸ¥¦ì‹í’ˆ&ê±´ê°•ì‹í’ˆ / ê±´ê°•ë³´ì¡°ì œ
            10. "ìš”ê°€ë§¤íŠ¸ì™€ ë¤ë²¨ ì„¸íŠ¸" â†’ ğŸ§´ë·°í‹°&í—¬ìŠ¤ / í—¬ìŠ¤&í”¼íŠ¸ë‹ˆìŠ¤
            11. "ë‹¤ìš©ë„ ì„ ë¬¼ì„¸íŠ¸(ì—¬ëŸ¬ ì¹´í…Œê³ ë¦¬ ìƒí’ˆ í˜¼í•©)" â†’ ğŸš—ê¸°íƒ€&ì „ìì œí’ˆ / ê¸°íƒ€
            12. "ìœ ê¸°ë† ì—‘ìŠ¤íŠ¸ë¼ ë²„ì§„ ì˜¬ë¦¬ë¸Œ ì˜¤ì¼" â†’ ğŸ¥¦ì‹í’ˆ&ê±´ê°•ì‹í’ˆ / ê°„í¸ì‹&ì¡°ë¯¸ë£Œ
            
            ì‘ë‹µ í˜•ì‹:
            {{
            "main_category": "ì´ëª¨ì§€ì™€ í•¨ê»˜ ì£¼ ì¹´í…Œê³ ë¦¬ëª…",
            "sub_category": "ì„œë¸Œ ì¹´í…Œê³ ë¦¬ëª…"
            }}
            
            JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”. ì˜ˆì‹œ ì‘ë‹µ:
            {{
            "main_category": "ğŸ½ì£¼ë°©ìš©í’ˆ&ì‹ê¸°",
            "sub_category": "ë„ë§ˆ,ì¡°ë¦¬ë„êµ¬,ì‹ê¸°ì„¸íŠ¸"
            }}
            """
            
            # OpenAI API í˜¸ì¶œ
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì •í™•í•˜ê³  ì¼ê´€ëœ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ê°€ ë‹¹ì‹ ì˜ ì£¼ìš” ì—…ë¬´ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                response_format={"type": "json_object"},
                temperature=0.2
            )
            
            # ì‘ë‹µ íŒŒì‹± - response.text ëŒ€ì‹  ì˜¬ë°”ë¥¸ ì†ì„± ì‚¬ìš©
            try:
                result = json.loads(response.choices[0].message.content)
                return result["main_category"], result["sub_category"]
            except json.JSONDecodeError:
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ ì¶”ì¶œ ì‹œë„
                text = response.choices[0].message.content
                main_match = re.search(r'"main_category":\s*"([^"]+)"', text)
                sub_match = re.search(r'"sub_category":\s*"([^"]+)"', text)
                
                if main_match and sub_match:
                    return main_match.group(1), sub_match.group(1)
                else:
                    print("ì‘ë‹µì—ì„œ ì¹´í…Œê³ ë¦¬ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return "", ""
    
        except Exception as e:
            # API í• ë‹¹ëŸ‰ ì´ˆê³¼ ì‹œ ì¬ì‹œë„ ë¡œì§ ì¶”ê°€
            if "429" in str(e):  # Resource exhausted ì—ëŸ¬ ì½”ë“œ
                retry_count += 1
                print(f"OpenAI API í• ë‹¹ëŸ‰ ì´ˆê³¼. {retry_delay}ì´ˆ í›„ ì¬ì‹œë„... (ì‹œë„ {retry_count}ë²ˆì§¸)")
                time.sleep(retry_delay)  # retry_delay ë³€ìˆ˜ ì‚¬ìš©
                continue  # ë£¨í”„ ì²˜ìŒìœ¼ë¡œ ëŒì•„ê°€ ì¬ì‹œë„
            else:
                # ë‹¤ë¥¸ ì¢…ë¥˜ì˜ ì˜¤ë¥˜ëŠ” ê¸°ì¡´ì²˜ëŸ¼ ì²˜ë¦¬
                print(f"OpenAI API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                return "", ""  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜

def analyze_instagram_feed():
    try:
        # í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ë¡œë“œ
        load_dotenv()
        
        # OpenAI API ì„¤ì • (Gemini ëŒ€ì‹  GPT-4o-mini ì‚¬ìš©)
        openai.api_key = os.getenv('OPENAI_API_KEY')
        client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        
        # MongoDB ì—°ê²°
        mongo_client, collections = get_mongodb_connection()
        feeds_collection = collections['feeds']
        
        # ì²˜ë¦¬ë˜ì§€ ì•Šì€ í”¼ë“œ ë°ì´í„° ì¡°íšŒ (í˜ì´ì§€ë„¤ì´ì…˜ ë°©ì‹ìœ¼ë¡œ ë³€ê²½)
        batch_size = 50  # í•œ ë²ˆì— ì²˜ë¦¬í•  ë¬¸ì„œ ìˆ˜
        total_feeds = feeds_collection.count_documents({
            '$or': [
                {'processed': False},
                {'processed': {'$exists': False}}
            ]
        })
        
        print(f"ì´ {total_feeds}ê°œì˜ ê²Œì‹œê¸€ì„ ë¶„ì„í•©ë‹ˆë‹¤...")
        
        processed_count = 0
        last_id = None
        
        while processed_count < total_feeds:
            # ID ê¸°ë°˜ í˜ì´ì§€ë„¤ì´ì…˜ ì¿¼ë¦¬
            query = {
                '$or': [
                    {'processed': False},
                    {'processed': {'$exists': False}}
                ]
            }
            
            if last_id:
                query['_id'] = {'$gt': last_id}
                
            # ë°°ì¹˜ í¬ê¸°ë§Œí¼ë§Œ ì¡°íšŒ
            batch = list(feeds_collection.find(query).sort('_id', 1).limit(batch_size))
            
            if not batch:
                break  # ë” ì´ìƒ ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŒ
                
            for i, item in enumerate(batch, start=processed_count+1):
                print(f"\n[{i}/{total_feeds}] {i}ë²ˆ ê²Œì‹œê¸€ ë¶„ì„ ì¤‘...")
                last_id = item['_id']  # ë§ˆì§€ë§‰ ID ì €ì¥
                
                try:
                    # API í• ë‹¹ëŸ‰ ì´ˆê³¼ ì‹œ ì¬ì‹œë„ ë¡œì§
                    retry_count = 0
                    wait_time = 10  # ì´ˆê¸° ëŒ€ê¸° ì‹œê°„ 10ì´ˆ
                    
                    while True:  # ë¬´í•œ ë£¨í”„ë¡œ ë³€ê²½
                        try:
                            # ì´ë¯¸ ì²˜ë¦¬ëœ ë°ì´í„° ì²´í¬ ë¡œì§
                            if item.get('09_feed'):
                                print(f"{i}ë²ˆ ê²Œì‹œê¸€: ì´ë¯¸ ì²˜ë¦¬ëœ ë°ì´í„°ì…ë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                                feeds_collection.update_one(
                                    {'_id': item['_id']},
                                    {'$set': {'processed': True}}
                                )
                                break  # ì²˜ë¦¬ ì™„ë£Œ ì‹œ ë£¨í”„ ì¢…ë£Œ

                            # ë³¸ë¬¸ ë‚´ìš© í™•ì¸
                            content = item.get('content')
                            author = item.get('author')

                            if not content or not content.strip():
                                if not author:  # authorë„ ë¹„ì–´ìˆëŠ” ê²½ìš°
                                    print(f"ë³¸ë¬¸ ë‚´ìš©ê³¼ ì‘ì„±ìê°€ ë¹„ì–´ìˆì–´ ì‚­ì œí•©ë‹ˆë‹¤. (ê²Œì‹œë¬¼ ë§í¬: {item['post_url']})")
                                    feeds_collection.delete_one({'_id': item['_id']})  # ë„íë¨¼íŠ¸ ì‚­ì œ
                                else:
                                    print(f"ë³¸ë¬¸ ë‚´ìš©ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. (ì‘ì„±ì: {author}, ê²Œì‹œë¬¼ ë§í¬: {item['post_url']})")
                                
                                # ê²Œì‹œê¸€ ì²˜ë¦¬ ìƒíƒœ ì—…ë°ì´íŠ¸
                                feeds_collection.update_one(
                                    {'_id': item['_id']},
                                    {'$set': {'processed': True}}  # ì²˜ë¦¬ ì™„ë£Œë¡œ ì„¤ì •
                                )
                                break  # ë‹¤ìŒ ê²Œì‹œë¬¼ë¡œ ë„˜ì–´ê°
                            
                            print(f"{i}ë²ˆ ê²Œì‹œê¸€: GPT-4o-mini ë¶„ì„ ìš”ì²­ ì¤‘...")
                            
                            # í”„ë¡¬í”„íŠ¸ ì‘ì„±
                            prompt = f"""
                            You are analyzing Instagram feed content. Extract information and respond in the following JSON format only:
                            {{
                                "is_group_buy": "ê³µêµ¬ì˜ˆê³ "/"ê³µêµ¬ì˜¤í”ˆ"/"ê³µêµ¬ë¦¬ë§ˆì¸ë“œ"/"í™•ì¸í•„ìš”"/"N",
                                "product_name": "Include all main product categories in the title",
                                "brand_name": "brand name here",
                                "start_date": "MM-DD format only if year is not specified",
                                "end_date": "MM-DD format only if year is not specified"
                            }}
                            
                            For group buy classification:
                            - "ê³µêµ¬ì˜ˆê³ ": Post announces future group buy with specific future date
                            - "ê³µêµ¬ì˜¤í”ˆ": Post announces group buy opening today or is currently open
                            - "ê³µêµ¬ë¦¬ë§ˆì¸ë“œ": Post reminds of ongoing group buy or last call
                            - "í™•ì¸í•„ìš”": Unclear whether it's a group buy or needs verification
                            - "N": Not a group buy post
                            
                            Important indicators:
                            - ê³µêµ¬ì˜ˆê³ : "ê³§ ì˜¤í”ˆ", "ì˜¤í”ˆ ì˜ˆì •", "Coming soon", "ì¤€ë¹„ì¤‘"
                            - ê³µêµ¬ì˜¤í”ˆ: "OPEN", "ì˜¤í”ˆ", "ğ‘¶ğ‘·ğ‘¬ğ‘µ", "open", "ì‹œì‘", "ì˜¤í”ˆí–ˆì–´ìš”"
                            - ê³µêµ¬ë¦¬ë§ˆì¸ë“œ: "ë§ˆê°ì„ë°•", "ì˜¤ëŠ˜ë§ˆê°", "ë§ˆì§€ë§‰", "ì¬ê³  ì–¼ë§ˆì—†ì–´ìš”"
                            
                            For dates:
                            - If year is not specified in the content, only extract MM-DD
                            - If post mentions "OPEN", "ì˜¤í”ˆ", "ğ‘¶ğ‘·ğ‘¬ğ‘µ", "open" without specific date, mark as "ë‹¹ì¼"
                            - For shipping dates (e.g., "2/7ì¼ë¶€í„° ìˆœì°¨ ë°œì†¡"), use post date as start_date
                            - Do not assume or add any year information
                            - Return empty string if no date is found
                            
                            Important indicators for group buy (ê³µêµ¬):
                            - Keywords like "OPEN", "ì˜¤í”ˆ", "ğ‘¶ğ‘·ğ‘¬ğ‘µ", "open"
                            - "í”„ë¡œí•„ ë§í¬ì—ì„œ êµ¬ë§¤"
                            - Detailed product information with pricing
                            - Limited time offer implications
                            - Comment inducement for purchase intention
                            - Specific purchase instructions or links
                            
                            Return "í™•ì¸í•„ìš”" when:
                            - Post contains some group buy indicators but lacks crucial information
                            - Unclear whether it's a product review or group buy
                            - Contains purchase-related keywords but no clear group buy format

                            For dates:
                            - If year is not specified in the content, only extract MM-DD
                            - If post mentions "OPEN" without specific date, mark as "ë‹¹ì¼"
                            - Do not assume or add any year information
                            - Return empty string if no date is found
                         
                            
                            Do not include any other text in your response.

                            Analyze the following Instagram post content:
                            {content}
                            """
                            
                            # GPT-4o-mini API í˜¸ì¶œ
                            response = client.chat.completions.create(
                                model="gpt-4o-mini",
                                messages=[
                                    {"role": "system", "content": "You are a content analyzer specializing in Instagram posts."},
                                    {"role": "user", "content": prompt}
                                ],
                                response_format={"type": "json_object"},
                                temperature=0.2
                            )
                            
                            try:
                                result = json.loads(response.choices[0].message.content)
                            except json.JSONDecodeError:
                                text = response.choices[0].message.content
                                json_match = re.search(r'({.*})', text.replace('\n', ''), re.DOTALL)
                                if json_match:
                                    try:
                                        result = json.loads(json_match.group(1))
                                    except:
                                        result = {
                                            "is_group_buy": "í™•ì¸í•„ìš”",
                                            "product_name": "",
                                            "brand_name": "",
                                            "start_date": "",
                                            "end_date": ""
                                        }
                                else:
                                    result = {
                                        "is_group_buy": "í™•ì¸í•„ìš”",
                                        "product_name": "",
                                        "brand_name": "",
                                        "start_date": "",
                                        "end_date": ""
                                    }

                            print(f"{i}ë²ˆ ê²Œì‹œê¸€: ë¶„ì„ ê²°ê³¼ ì‚¬ìš©")
                            
                            # 'ë‹¹ì¼' ì²˜ë¦¬
                            if result['start_date'] == 'ë‹¹ì¼':
                                date_parts = item['cr_at'].split('T')[0].split('-')
                                result['start_date'] = f"{date_parts[1]}-{date_parts[2]}"
                                
                            if result['end_date'] == 'ë‹¹ì¼':
                                date_parts = item['cr_at'].split('T')[0].split('-')
                                result['end_date'] = f"{date_parts[1]}-{date_parts[2]}"

                            # ë¸Œëœë“œëª…ì´ ë¹„ì–´ìˆê³  ìƒí’ˆëª…ì´ ìˆëŠ” ê²½ìš° 'í™•ì¸í•„ìš”'ë¡œ ì„¤ì •
                            if not result['brand_name'] and result['product_name']:
                                result['brand_name'] = 'í™•ì¸í•„ìš”'

                            # GPT-4o-mini ì‘ë‹µ ì¶œë ¥
                            print(f"{i}ë²ˆ ê²Œì‹œê¸€: GPT-4o-mini ì‘ë‹µ ë‚´ìš©: {json.dumps(result, ensure_ascii=False, indent=4)} (ì‘ì„±ì: {item['author']}, ê²Œì‹œë¬¼ ë§í¬: {item['post_url']})")
                            
                            # ë‚ ì§œ í˜•ì‹ ê²€ì¦ ë° ì •ë¦¬ í•¨ìˆ˜
                            def validate_date(date_str, created_date=None):
                                if not date_str or date_str.strip() == '':
                                    return ''
                                try:
                                    if date_str == 'ë‹¹ì¼' and created_date:
                                        # created_dateì—ì„œ ì›”-ì¼ë§Œ ì¶”ì¶œ (MM-DD í˜•ì‹)
                                        date_parts = created_date.split('T')[0].split('-')
                                        return f"{date_parts[1]}-{date_parts[2]}"
                                    
                                    # MM-DD í˜•ì‹ì„ ì²˜ë¦¬
                                    date_str = date_str.replace('/', '-').replace('.', '-')
                                    parts = date_str.split('-')
                                    
                                    if len(parts) == 2:
                                        # created_dateì—ì„œ ì—°ë„ ì¶”ì¶œ
                                        year = created_date.split('T')[0].split('-')[0] if created_date else "2024"
                                        month = parts[0].zfill(2)
                                        day = parts[1].zfill(2)
                                        return f"{year}-{month}-{day}"
                                    elif len(parts) == 3:
                                        year = parts[0]
                                        if len(year) == 2:
                                            year = f"20{year}"
                                        month = parts[1].zfill(2)
                                        day = parts[2].zfill(2)
                                        return f"{year}-{month}-{day}"
                                    return ''
                                except:
                                    return ''

                            # ë‚ ì§œ ê²€ì¦ ë° ì²˜ë¦¬
                            created_date = item.get('cr_at')
                            
                            update_data = {}
                            
                            if result['is_group_buy'] in ['ê³µêµ¬ì˜ˆê³ ', 'ê³µêµ¬ì˜¤í”ˆ', 'ê³µêµ¬ë¦¬ë§ˆì¸ë“œ']:
                                # ì‹œì‘ì¼ ì²˜ë¦¬ - ì´ì œ 'ë‹¹ì¼' ì²˜ë¦¬ëŠ” ìœ„ì—ì„œ ì™„ë£Œë¨
                                start_date = validate_date(str(result['start_date']), created_date)
                                end_date = validate_date(str(result['end_date']), created_date)
                                
                                # ë¸Œëœë“œëª… ì •ê·œí™” ì²˜ë¦¬ ì¶”ê°€
                                normalized_brand = normalize_brand(result['brand_name'], collections['brands'], item['author'])
                                
                                update_data = {
                                    '09_feed': result['is_group_buy'],
                                    '09_item': str(result['product_name']),
                                    '09_brand': str(normalized_brand['name']),  # ì •ê·œí™”ëœ ë¸Œëœë“œëª… ì‚¬ìš©
                                    'open_date': start_date,
                                    'end_date': end_date,
                                    '09_item_category': '',
                                    '09_item_category_2': '',
                                    'processed': True
                                }
                                
                                # GPT-4o-mini APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì¹´í…Œê³ ë¦¬ ë¶„ì„
                                if result['product_name']:
                                    try:
                                        item_category, item_category2 = analyze_product_category(result['product_name'])
                                        update_data['09_item_category'] = item_category
                                        update_data['09_item_category_2'] = item_category2
                                        
                                        # ì¹´í…Œê³ ë¦¬ ë¶„ì„ ê²°ê³¼ ì¶œë ¥
                                        print(f"{i}ë²ˆ ê²Œì‹œê¸€: ì¹´í…Œê³ ë¦¬ ë¶„ì„ ê²°ê³¼ - ì£¼ ì¹´í…Œê³ ë¦¬: {item_category}, ì„œë¸Œ ì¹´í…Œê³ ë¦¬: {item_category2}")
                                    except Exception as e:
                                        print(f"ì¹´í…Œê³ ë¦¬ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                                
                                # MongoDB ì—…ë°ì´íŠ¸
                                feeds_collection.update_one(
                                    {'_id': item['_id']},
                                    {'$set': update_data}
                                )
                                
                                # ì¸í”Œë£¨ì–¸ì„œ ë°ì´í„° ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•´ item ì—…ë°ì´íŠ¸
                                item.update(update_data)
                                update_influencer_data(item, collections)
                            
                            else:
                                update_data = {
                                    '09_feed': 'N' if result['is_group_buy'] == 'N' else 'í™•ì¸í•„ìš”',
                                    '09_item': '',
                                    '09_brand': '',
                                    'open_date': '',
                                    'end_date': '',
                                    '09_item_category': '',
                                    '09_item_category_2': '',
                                    'processed': True
                                }
                                
                                feeds_collection.update_one(
                                    {'_id': item['_id']},
                                    {'$set': update_data}
                                )
                            
                            # ê° ê²Œì‹œê¸€ ë¶„ì„ ì™„ë£Œ í›„ 1ì´ˆ ëŒ€ê¸°
                            print(f"{i}ë²ˆ ê²Œì‹œê¸€: ì²˜ë¦¬ ì™„ë£Œ")
                            
                            break  # ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ë©´ while ë£¨í”„ ì¢…ë£Œ
                            
                        except Exception as e:
                            if "429" in str(e):  # Resource exhausted ì—ëŸ¬
                                retry_count += 1
                                print(f"\nOpenAI API í• ë‹¹ëŸ‰ ì´ˆê³¼. {wait_time}ì´ˆ í›„ ì¬ì‹œë„... (ì‹œë„ {retry_count})")
                                time.sleep(wait_time)  # 10ì´ˆ ëŒ€ê¸°
                            else:
                                # ë‹¤ë¥¸ ì¢…ë¥˜ì˜ ì˜¤ë¥˜ëŠ” ê¸°ì¡´ì²˜ëŸ¼ ì²˜ë¦¬
                                print(f"{i}ë²ˆ ê²Œì‹œê¸€: ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ - {str(e)}")
                                break  # ë‹¤ë¥¸ ì˜¤ë¥˜ ë°œìƒ ì‹œ ë£¨í”„ ì¢…ë£Œ
                
                except Exception as e:
                    print(f"{i}ë²ˆ ê²Œì‹œê¸€: ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ - {str(e)}")
                    continue
            
            processed_count += len(batch)
            print(f"í˜„ì¬ê¹Œì§€ {processed_count}/{total_feeds} ê²Œì‹œê¸€ ì²˜ë¦¬ ì™„ë£Œ")
        
        print("\nëª¨ë“  ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        mongo_client.close()
        
    except Exception as e:
        print(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

def setup_brand_logger():
    """ë¸Œëœë“œ ì •ê·œí™” ë¡œê¹… ì„¤ì •"""
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    log_dir = "brand_logs"
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)
    
    # ë¡œê·¸ íŒŒì¼ëª… ìƒì„± (ê³ ì •ëœ ì´ë¦„)
    log_filename = os.path.join(log_dir, "brand_normalization.log")
    
    # ë¡œê±° ì„¤ì •
    logger = logging.getLogger('brand_normalizer')
    logger.setLevel(logging.INFO)
    
    # íŒŒì¼ í•¸ë“¤ëŸ¬ (append ëª¨ë“œ)
    file_handler = logging.FileHandler(log_filename, encoding='utf-8', mode='a')
    file_handler.setLevel(logging.INFO)
    
    # í¬ë§· ì„¤ì •
    formatter = logging.Formatter('%(message)s')
    file_handler.setFormatter(formatter)
    
    # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±° (ì¤‘ë³µ ë¡œê¹… ë°©ì§€)
    if logger.hasHandlers():
        logger.handlers.clear()
    
    # í•¸ë“¤ëŸ¬ ì¶”ê°€
    logger.addHandler(file_handler)
    
    return logger

def setup_log_directory():
    """ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±"""
    log_dir = "brand_logs"
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)
    return log_dir

def normalize_brand(brand_name, brands_collection, author=None):
    """ë¸Œëœë“œëª… ì •ê·œí™” ë° ë³‘í•© ì²˜ë¦¬"""
    try:
        logger = setup_brand_logger()
        
        logger.info(f"\n{'='*50}")
        logger.info(f"ê²€ì‚¬ ëŒ€ìƒ ë¸Œëœë“œ: '{brand_name}'")
        if author:
            logger.info(f"ì‘ì„±ì: '{author}'")
        logger.info(f"ê²€ì‚¬ ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        
        # None, Unspecified, ë¹ˆ ê°’ ì²˜ë¦¬
        if not brand_name or brand_name in ['None', 'Unspecified', 'í™•ì¸í•„ìš”']:
            logger.info("âš ï¸ ë¸Œëœë“œëª…ì´ ì—†ê±°ë‚˜ ë¯¸ì§€ì • ìƒíƒœì…ë‹ˆë‹¤.")
            logger.info(f"{'='*50}\n")
            return {'name': 'í™•ì¸í•„ìš”', 'category': ''}
        
        # ìœ ì‚¬ë„ ë†’ì€ ë¸Œëœë“œë“¤ 1ì°¨ ì¶”ì¶œ
        similar_brands = []
        all_brands = list(brands_collection.find())
        
        for brand in all_brands:
            similarity = jaro_winkler_similarity(brand_name.lower(), brand['name'].lower())
            if similarity >= 0.8:
                similar_brands.append((brand['name'], brand, similarity))
        
        # ìœ ì‚¬ ë¸Œëœë“œ ë¡œê¹…
        if not similar_brands:
            logger.info("ğŸ“¢ ìœ ì‚¬ë„ 0.8 ì´ìƒì¸ ë¸Œëœë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
            logger.info(f"{'='*50}\n")
            return {'name': brand_name, 'category': ''}
        
        logger.info("\nğŸ“‹ ìœ ì‚¬ë„ 0.8 ì´ìƒ í›„ë³´êµ°:")
        for brand, info, similarity in similar_brands:
            logger.info(f"- {brand} (ìœ ì‚¬ë„: {similarity:.4f})")
            if info.get('aliases'):
                logger.info(f"  â”” ê¸°ì¡´ ë³„ì¹­: {', '.join(info['aliases'])}")
        
        # OpenAI GPT-4o-mini ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë¸Œëœë“œ ê´€ê³„ ë¶„ì„
        if similar_brands:
            analysis = analyze_brands_with_openai(brand_name, similar_brands[:10])
            
            if analysis and analysis.get('representative_brand'):
                logger.info("\nğŸ¤– OpenAI ë¶„ì„ ê²°ê³¼:")
                logger.info(f"ëŒ€í‘œ ë¸Œëœë“œ: '{analysis['representative_brand']}'")
                logger.info(f"ë³„ì¹­ìœ¼ë¡œ ì²˜ë¦¬: {analysis.get('aliases', [])}")
                if analysis.get('different_brands'):
                    logger.info(f"ë‹¤ë¥¸ ë¸Œëœë“œë¡œ ì¸ì‹: {analysis['different_brands']}")
                
                rep_brand = analysis['representative_brand']
                new_aliases = analysis.get('aliases', [])
                
                # ëŒ€í‘œ ë¸Œëœë“œ ë¬¸ì„œ ì—…ë°ì´íŠ¸
                rep_doc = brands_collection.find_one({'name': rep_brand})
                if rep_doc:
                    # ê¸°ì¡´ ë³„ì¹­ë“¤ë„ í¬í•¨í•˜ì—¬ ë³‘í•©
                    existing_aliases = []
                    for brand, info, _ in similar_brands:
                        if brand in new_aliases:  # OpenAIê°€ ë³„ì¹­ìœ¼ë¡œ íŒë‹¨í•œ ë¸Œëœë“œì˜
                            existing_aliases.extend(info.get('aliases', []))  # ê¸°ì¡´ ë³„ì¹­ë“¤ë„ ì¶”ê°€
                
                    # merge_aliases í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  ë³„ì¹­ ë³‘í•©
                    merged_aliases = merge_aliases(
                        rep_doc.get('aliases', []),  # ëŒ€í‘œ ë¸Œëœë“œì˜ ê¸°ì¡´ ë³„ì¹­
                        existing_aliases + new_aliases,  # ìƒˆë¡œìš´ ë³„ì¹­ë“¤ê³¼ ê·¸ë“¤ì˜ ê¸°ì¡´ ë³„ì¹­
                        brand_name  # í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ ë¸Œëœë“œ
                    )
                    
                    # '-' ì œê±° ë¡œì§ ì¶”ê°€
                    cleaned_aliases = [alias.strip('- ') for alias in merged_aliases]

                    # MongoDB ì—…ë°ì´íŠ¸
                    brands_collection.update_one(
                        {'name': rep_brand},
                        {'$set': {
                            'aliases': cleaned_aliases,  # '-' ì œê±°ëœ ë³„ì¹­ ì‚¬ìš©
                            'status': 'done'
                        }}
                    )
                    
                    # ë³„ì¹­ ë¸Œëœë“œë“¤ ì‚­ì œ
                    for alias in new_aliases:
                        brands_collection.delete_one({'name': alias})
                    
                    logger.info(f"ìµœì¢… ë³‘í•©ëœ ë³„ì¹­ ëª©ë¡: {cleaned_aliases}")  # ë¡œê¹… ì¶”ê°€
                    
                    return {
                        'name': rep_brand,
                        'category': rep_doc.get('category', '')
                    }
                
                # rep_docì´ ì—†ëŠ” ê²½ìš°ì˜ ì²˜ë¦¬ ì¶”ê°€
                else:
                    # ëŒ€í‘œ ë¸Œëœë“œë¡œ ìƒˆ ë¬¸ì„œ ìƒì„±
                    new_brand = {
                        'name': rep_brand,
                        'aliases': new_aliases,  # ì´ˆê¸°í™”ëœ ë³„ì¹­ ì‚¬ìš©
                        'status': 'done'
                    }
                    brands_collection.insert_one(new_brand)
                    
                    # ë³„ì¹­ ë¸Œëœë“œë“¤ ì‚­ì œ
                    for alias in new_aliases:
                        brands_collection.delete_one({'name': alias})
                    
                    logger.info(f"ìƒˆ ë¸Œëœë“œë¡œ ë“±ë¡: {rep_brand} (ë³„ì¹­: {new_aliases})")
                    return {
                        'name': rep_brand,
                        'category': ''
                    }
        
        # ìƒˆ ë¸Œëœë“œ ë“±ë¡
        new_brand = {
            'name': brand_name,
            'category': '',
            'aliases': [brand_name],
            'level': '',
            'status': 'ready'
        }
        brands_collection.insert_one(new_brand)
        
        logger.info("\nğŸ“ ìƒˆ ë¸Œëœë“œ ë“±ë¡:")
        logger.info(f"ë¸Œëœë“œëª…: '{brand_name}'")
        logger.info(f"{'='*50}\n")
        
        return {'name': brand_name, 'category': ''}
        
    except Exception as e:
        error_message = f"ë¸Œëœë“œ ì •ê·œí™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        logger.error(f"\nâŒ {error_message}")
        logger.info(f"{'='*50}\n")
        print(error_message)
        return {'name': brand_name, 'category': ''}

def analyze_brands_with_openai(target_brand, similar_brands):
    """OpenAI GPT-4o-mini ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë¸Œëœë“œ ê´€ê³„ ë¶„ì„"""
    max_retries = 10
    retry_count = 0
    wait_time = 10
    
    while retry_count < max_retries:
        try:
            prompt = f"""
ë‹¹ì‹ ì€ ë¸Œëœë“œ ë¶„ë¥˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë¸Œëœë“œì˜ ì˜ë¯¸ì™€ ë§¥ë½ì„ ì •í™•íˆ ì´í•´í•˜ê³  ë¶„ì„í•´ì•¼ í•©ë‹ˆë‹¤.

ì´ ì‘ì—…ì˜ ëª©ì ì€ ì¸ìŠ¤íƒ€ê·¸ë¨ì—ì„œ ìˆ˜ì§‘ëœ ë¸Œëœë“œ ë°ì´í„°ë¥¼ ì •ë¦¬í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
ê°™ì€ ë¸Œëœë“œê°€ ë‹¤ì–‘í•œ í˜•íƒœë¡œ í‘œê¸°ë˜ì–´ ìˆì–´, ì´ë¥¼ í•˜ë‚˜ì˜ ëŒ€í‘œ ë¸Œëœë“œëª…ìœ¼ë¡œ í†µì¼í•˜ê³  
ë‚˜ë¨¸ì§€ëŠ” ë³„ì¹­ìœ¼ë¡œ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.
ì§€ê¸ˆ ê²€ì‚¬í•˜ëŠ” ë¸Œëœë“œëŠ” "{target_brand}"ì…ë‹ˆë‹¤.
ì´ ë¸Œëœë“œì™€ ìœ ì‚¬ë„ê°€ ë†’ì€ ë¸Œëœë“œë“¤ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

{[f"- {brand} (ìœ ì‚¬ë„: {similarity:.3f})" for brand, _, similarity in similar_brands]}

ì£¼ì˜ì‚¬í•­:
1. ê²€ì‚¬ ëŒ€ìƒ ë¸Œëœë“œëª…("{target_brand}")ì„ ì ˆëŒ€ ë³€ê²½í•˜ì§€ ë§ˆì„¸ìš”.
2. ê²€ì‚¬ ëŒ€ìƒ ë¸Œëœë“œê°€ ëŒ€í‘œ ë¸Œëœë“œê°€ ë  ìˆ˜ë„ ìˆê³ , ë‹¤ë¥¸ ë¸Œëœë“œì˜ ë³„ì¹­ì´ ë  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
3. ìœ ì‚¬ë„ë§Œìœ¼ë¡œ íŒë‹¨í•˜ì§€ ë§ê³ , ë¸Œëœë“œì˜ ì˜ë¯¸ì™€ ë§¥ë½ì„ ê³ ë ¤í•˜ì„¸ìš”.
4. í™•ì‹¤í•˜ì§€ ì•Šì€ ê²½ìš° ë‹¤ë¥¸ ë¸Œëœë“œë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.

ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ì˜ JSONìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "representative_brand": "string",  # ëŒ€í‘œ ë¸Œëœë“œë¡œ ì„ ì •ëœ ì´ë¦„
    "aliases": ["string"],            # í™•ì‹¤íˆ ê°™ì€ ë¸Œëœë“œì¸ ê²½ìš°ë§Œ ë³„ì¹­ìœ¼ë¡œ ì²˜ë¦¬
    "different_brands": ["string"]    # ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ê²½ìš° ëª¨ë‘ ë‹¤ë¥¸ ë¸Œëœë“œë¡œ ì œì™¸
}}
"""
            
            # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
            
            # GPT-4o-mini API í˜¸ì¶œ
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}]
            )
            
            text = response.choices[0].message.content.strip()
            
            # JSON í˜•ì‹ ì •ë¦¬
            if text.startswith('```json'):
                text = text[7:]
            if text.endswith('```'):
                text = text[:-3]
            text = text.strip()
            
            result = json.loads(text)
            return result
            
        except Exception as e:
            if "429" in str(e):  # API í• ë‹¹ëŸ‰ ì´ˆê³¼ ì˜¤ë¥˜
                retry_count += 1
                print(f"\nOpenAI API í• ë‹¹ëŸ‰ ì´ˆê³¼. {wait_time}ì´ˆ ëŒ€ê¸° ì¤‘... (ì‹œë„ {retry_count}/{max_retries})")
                time.sleep(wait_time)
                wait_time += 10
                continue
            else:
                print(f"OpenAI API ì˜¤ë¥˜: {e}")
                return None
    
    print(f"\nìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜({max_retries})ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.")
    return None

def normalize_brand_name(name):
    """ë¸Œëœë“œëª… ì •ê·œí™” (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì• ê³ , ê³µë°± ì œê±°)"""
    return name.lower().strip()

def merge_aliases(main_aliases, sub_aliases, sub_brand):
    """aliases ë³‘í•© ì‹œ ì¤‘ë³µ ì œê±° ë° ì •ê·œí™”"""
    # ëª¨ë“  ë³„ì¹­ì„ ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ì—¬ setì— ì €ì¥
    normalized_aliases = {normalize_brand_name(alias) for alias in main_aliases}
    
    # ìƒˆë¡œìš´ ë³„ì¹­ë“¤ ì¶”ê°€
    for alias in sub_aliases:
        normalized_alias = normalize_brand_name(alias)
        if normalized_alias not in normalized_aliases:
            normalized_aliases.add(normalized_alias)
    
    # ì„œë¸Œ ë¸Œëœë“œëª…ë„ ë³„ì¹­ìœ¼ë¡œ ì¶”ê°€
    normalized_sub_brand = normalize_brand_name(sub_brand)
    if normalized_sub_brand not in normalized_aliases:
        normalized_aliases.add(normalized_sub_brand)
    
    return list(normalized_aliases)

if __name__ == "__main__":
    analyze_instagram_feed()
